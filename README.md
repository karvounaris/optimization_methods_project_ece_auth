# ğŸ” Optimization Methods Project

## ğŸ“– Overview
This repository contains three assignments completed as part of the Optimization Methods course at Aristotle University of Thessaloniki. Each assignment focuses on different optimization techniques and their application to specific problems. The assignments were implemented using MATLAB, and the results showcase the effectiveness and limitations of various optimization algorithms.

---

## ğŸ§© Assignment 1: Unconstrained Optimization Methods

### ğŸ¯ Goals
- Compare and analyze the performance of the following methods:
  - **ğŸ” Dichotomous Search**
  - **ğŸ“ Golden Section Search**
  - **ğŸ”¢ Fibonacci Search**
  - **ğŸ“‰ Derivative-Based Dichotomous Search**
- Evaluate their convergence behavior for different functions and parameter settings.

### âœ¨ Features
- Application of the methods to three distinct functions.
- Analysis of the impact of varying step sizes (`l`) and tolerances (`Îµ`) on convergence.
- Visualizations of convergence patterns and computational efficiency.

### ğŸ† Results
- The methods provided consistent results across different functions, with faster convergence observed for larger step sizes (`l`).
- The Golden Section and Fibonacci methods showed similar performance, with slight variations in computational steps.
- Derivative-based methods highlighted the role of function properties in algorithmic efficiency.

---

## ğŸ§© Assignment 2: Constrained Optimization with Gradient-Based Methods

### ğŸ¯ Goals
- Optimize the objective function: 

  	**f(x, y) = x^5 * exp(-xÂ² - yÂ²)**

- Compare the performance of:
  - **ğŸ“‰ Steepest Descent**
  - **ğŸ“Š Newton's Method**
  - **âš™ï¸ Levenbergâ€“Marquardt Algorithm**

### âœ¨ Features
- Analysis of convergence from different initial points.
- Evaluation of step-size selection strategies:
  - Fixed step size
  - Line search minimizing `f(xk + Î³k * dk)`
  - Armijo rule-based step size.
- Application to both constrained and unconstrained scenarios.

### ğŸ† Results
- **Steepest Descent**: Effective for some initial points but sensitive to local optima.
- **Newton's Method**: Limited by the requirement of a positive definite Hessian matrix.
- **Levenbergâ€“Marquardt Algorithm**: Improved robustness, converging to global optima in challenging scenarios.

---

## ğŸ§© Assignment 3: Optimization with Equality and Inequality Constraints

### ğŸ¯ Goals
- Minimize the objective function: 

  	**f(x1, x2) = (1/3)x1Â² + 3x2Â²**

  Subject to constraints: 
  - **-10 â‰¤ x1 â‰¤ 5**
  - **-8 â‰¤ x2 â‰¤ 12**

- Explore the behavior of the Steepest Descent algorithm under varying constraints and parameter settings.

### âœ¨ Features
- Simulation of unconstrained and constrained optimization scenarios.
- Analysis of parameter sensitivity (e.g., `sk`, `Î³k`, and initial points).
- Implementation of custom adjustments to ensure feasibility within constraints.

### ğŸ† Results
- Parameter tuning is crucial for convergence under constraints.
- Inappropriate step size (`sk`) led to divergence or suboptimal solutions.
- Adjustments to ensure feasibility improved the algorithmâ€™s reliability.

---

## ğŸ“‚ Repository Contents
- **ğŸ“„ Assignment Reports**:
  - [Assignment 1 Report](./Project_1.pdf)
  - [Assignment 2 Report](./Project_2.pdf)
  - [Assignment 3 Report](./Project_3.pdf)
- **ğŸ’» Code**: MATLAB scripts for the implementation and analysis of optimization methods.

---

Thank you for exploring this project! ğŸš€ Feel free to raise issues or contribute to improve the repository. ğŸ˜Š
